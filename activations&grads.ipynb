{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTpjHrvBSndTp5tWMefgHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushichadha/multiLayerPerceptron/blob/main/activations%26grads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Studying internals of MLPs with multiple layers and scrutinizing the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. Also, looking at the typical diagnostic tools and visualizations we'd want to use to understand the health of our deep network. We learn why training deep neural nets can be fragile and introducing the first modern innovation that made doing so much easier: Batch Normalization."
      ],
      "metadata": {
        "id": "YzEgYbjEHbuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading materials-"
      ],
      "metadata": {
        "id": "qMAS6e9aHrCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Neural Probabilistic Language Model paper (bengio et al.) - MLP\n",
        "*   Kaiming init Paper\n",
        "*   Batch Normalisation paper\n",
        "*   Rethinking \"batch\" in Batch Norm paper\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6xBzoEQiHwH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApaytuCSHPTU"
      },
      "outputs": [],
      "source": [
        "# fixing the initial loss\n",
        "# fixing the saturated tanh\n",
        "# calculating the init scale: “Kaiming init”\n",
        "# batch normalization\n",
        "# real example: resnet50 walkthrough\n",
        "# part2: PyTorch-ifying the code\n",
        "# viz #1: forward pass activations statistics\n",
        "# viz #2: backward pass gradient statistics\n",
        "# the fully linear case of no non-linearities\n",
        "# viz #3: parameter activation and gradient statistics\n",
        "# viz #4: update:data ratio over time\n",
        "# bringing back batchnorm, looking at the visualizations"
      ]
    }
  ]
}